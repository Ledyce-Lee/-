{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a372186f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/machine_learning/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "import pkuseg\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31863a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def load_reports_mda(base_folder: str = './2000-2023/') -> dict:\n",
    "\n",
    "    def extract_mda(text: str) -> str:\n",
    "        m = re.search(r'(管理层讨论与分析|MD&A)', text)\n",
    "        if not m:\n",
    "            return \"\"\n",
    "        tail = text[m.start():]\n",
    "        end_match = re.search(r'\\n[一二三四五六七八九十]+、|\\\\n[A-Z ]{5,}\\n', tail)\n",
    "        end = end_match.start() if end_match else len(tail)\n",
    "        return tail\n",
    "\n",
    "    mda_texts = {}\n",
    "    for root, _, files in os.walk(base_folder):\n",
    "\n",
    "        for fname in tqdm(files, desc=f\"Loading MD&A in {os.path.basename(root)}\"):\n",
    "            if not fname.endswith('.txt'):\n",
    "                continue\n",
    "            key = fname.rsplit('.txt', 1)[0]   # e.g. \"300097-智云股份-2013\"\n",
    "            path = os.path.join(root, fname)\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            mda = extract_mda(text)\n",
    "            if mda:\n",
    "                mda_texts[key] = mda\n",
    "    return mda_texts\n",
    "\n",
    "\n",
    "def segment_documents(texts: dict, model_name: str = 'web') -> dict:\n",
    "    \"\"\"\n",
    "    使用 pkuseg 对每个 MD&A 文本切词，\n",
    "    并显示分词进度。\n",
    "    返回：{ticker_year: [token1, token2, ...]}\n",
    "    \"\"\"\n",
    "    seg = pkuseg.pkuseg(model_name=model_name)\n",
    "    tokenized = {}\n",
    "\n",
    "    for ticker, doc in tqdm(texts.items(), desc=\"Tokenizing documents\"):\n",
    "        toks = seg.cut(doc)\n",
    "\n",
    "        tokenized[ticker] = [tok for tok in toks if tok.strip()]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95187979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def build_static_embeddings(model_name: str = \"langboat/moss-moon-003-sft\"):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    model = AutoModel.from_pretrained(model_name).eval()\n",
    "    \n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "\n",
    "    weight = model.get_input_embeddings().weight.detach().cpu().numpy()\n",
    "    \n",
    "\n",
    "    vocab = tokenizer.get_vocab()  # dict: token_str -> token_id\n",
    "    wp_emb = {}\n",
    "    for tok, idx in tqdm(list(vocab.items()),\n",
    "                         desc=\"Building static embeddings\",\n",
    "                         unit=\"token\",\n",
    "                         total=len(vocab)):\n",
    "        wp_emb[tok] = weight[idx]\n",
    "    \n",
    "    return tokenizer, wp_emb\n",
    "\n",
    "# === 基于 Transformer 扩充并获取相似度评分 ===\n",
    "\n",
    "def load_stop_words(path: str = 'stop_words.txt') -> set:\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return {line.strip() for line in f if line.strip()}\n",
    "    \n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def expand_seed_with_pmi_multi(\n",
    "    seed_words,\n",
    "    tokenized_docs,\n",
    "    tokenizer,\n",
    "    wp_emb,\n",
    "    topn=100,\n",
    "    center_k=100,\n",
    "    per_seed_k=50,\n",
    "    sim_thresh=0.01,\n",
    "    pmi_thresh=0.1,\n",
    "    stop_words_path='stop_words.txt'\n",
    "):\n",
    "  \n",
    "    seed_vecs = []\n",
    "    for w in seed_words:\n",
    "        pieces = tokenizer.tokenize(w)\n",
    "        vecs   = [wp_emb[p] for p in pieces if p in wp_emb]\n",
    "        if vecs:\n",
    "            seed_vecs.append(np.mean(vecs, axis=0))\n",
    "    if not seed_vecs:\n",
    "        return []\n",
    "    center = np.mean(seed_vecs, axis=0)\n",
    "    center_norm = np.linalg.norm(center)\n",
    "\n",
    "    tokens     = [t for t in wp_emb.keys() if not t.startswith('##')]\n",
    "    emb_matrix = np.stack([wp_emb[t] for t in tokens])\n",
    "    norms      = np.linalg.norm(emb_matrix, axis=1)\n",
    "\n",
    "    cos_center = emb_matrix.dot(center) / (norms * center_norm + 1e-12)\n",
    "    idx_center = np.argsort(cos_center)[::-1][:center_k]\n",
    "    center_candidates = { tokens[i] for i in idx_center }\n",
    "\n",
    "    seed_candidates = set()\n",
    "    for w in seed_words:\n",
    "        pieces = tokenizer.tokenize(w)\n",
    "        vecs   = [wp_emb[p] for p in pieces if p in wp_emb]\n",
    "        if not vecs:\n",
    "            continue\n",
    "        seed_vec   = np.mean(vecs, axis=0)\n",
    "        seed_norm  = np.linalg.norm(seed_vec)\n",
    "        cos_seed   = emb_matrix.dot(seed_vec) / (norms * seed_norm + 1e-12)\n",
    "        idxs       = np.argsort(cos_seed)[::-1][:per_seed_k]\n",
    "        for i in idxs:\n",
    "            seed_candidates.add(tokens[i])\n",
    "\n",
    "    all_candidates = center_candidates.union(seed_candidates)\n",
    "\n",
    "    N = len(tokenized_docs)\n",
    "    inverted_index = {}\n",
    "    df = Counter()\n",
    "    for doc_id, toks in tokenized_docs.items():\n",
    "        unique = set(toks)\n",
    "        for t in unique:\n",
    "            df[t] += 1\n",
    "            inverted_index.setdefault(t, set()).add(doc_id)\n",
    "\n",
    "    filtered_pmi = []\n",
    "    for w in tqdm(all_candidates, desc=\"PMI filtering\"):\n",
    "        df_w = df.get(w, 0)\n",
    "        if df_w == 0:\n",
    "            continue\n",
    "        docs_w = inverted_index[w]\n",
    "        for seed in seed_words:\n",
    "            df_s = df.get(seed, 0)\n",
    "            if df_s == 0:\n",
    "                continue\n",
    "            docs_s = inverted_index[seed]\n",
    "            co = len(docs_w & docs_s)\n",
    "            if co == 0:\n",
    "                continue\n",
    "            pmi = math.log((co * N) / (df_w * df_s) + 1e-12)\n",
    "            if pmi >= pmi_thresh:\n",
    "                # 使用中心相似度作为排序依据\n",
    "                sim_score = float(cos_center[tokens.index(w)])\n",
    "                filtered_pmi.append((w, sim_score))\n",
    "                break\n",
    "\n",
    "    filtered_pmi.sort(key=lambda x: x[1], reverse=True)\n",
    "    sims_sorted = filtered_pmi[:topn]\n",
    "\n",
    "    STOP_WORDS = load_stop_words(stop_words_path)\n",
    "    final = []\n",
    "    for w, s in sims_sorted:\n",
    "        if re.match(r'^[\\u4e00-\\u9fffA-Za-z0-9]+$', w) \\\n",
    "           and len(w) > 1 and w not in STOP_WORDS:\n",
    "            final.append((w, s))\n",
    "\n",
    "    idx = 0\n",
    "    while len(final) < topn and idx < len(sims_sorted):\n",
    "        w, s = sims_sorted[idx]\n",
    "        if re.match(r'^[\\u4e00-\\u9fffA-Za-z0-9]+$', w) \\\n",
    "           and len(w) > 1 and w not in STOP_WORDS \\\n",
    "           and (w, s) not in final:\n",
    "            final.append((w, s))\n",
    "        idx += 1\n",
    "\n",
    "    return final[:topn]\n",
    "\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def compute_idf(tokenized_docs: dict) -> dict: #模块 Y：计算 IDF 权重\n",
    "\n",
    "    N  = len(tokenized_docs)\n",
    "    df = Counter()\n",
    "    for tokens in tokenized_docs.values():\n",
    "        for w in set(tokens):\n",
    "            df[w] += 1\n",
    "    idf = {w: math.log((N + 1) / (df[w] + 1)) + 1 for w in df}\n",
    "    return idf\n",
    "\n",
    "def assign_weights_auto(expanded_sims: list, idf_dict: dict) -> dict:\n",
    "\n",
    "    idfs    = [idf_dict.get(w, 0.0) for w, _ in expanded_sims]\n",
    "    max_idf = max(idfs) or 1.0\n",
    "    weights = {}\n",
    "    for w, sim in expanded_sims:\n",
    "        idf_norm     = idf_dict.get(w, 0.0) / max_idf\n",
    "        weights[w]   = sim * idf_norm\n",
    "    return weights\n",
    "\n",
    "from collections import Counter as _Counter\n",
    "\n",
    "def compute_ai_scores(tokenized_docs: dict, dict_weights: dict) -> dict:\n",
    "\n",
    "    scores = {}\n",
    "    for ticker, tokens in tokenized_docs.items():\n",
    "        tf   = _Counter(tokens)\n",
    "        raw  = sum(dict_weights.get(w, 0.0) * tf.get(w, 0) for w in tokens)\n",
    "        norm = raw / max(len(tokens), 1)\n",
    "        scores[ticker] = math.log1p(norm)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204733dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已加载 57 个种子词： ['人工智能', '计算机视觉', '图像识别', '知识图谱', '增强现实', '特征提取', '支持向量机', '知识表示', '模式识别', '物联网'] …\n"
     ]
    }
   ],
   "source": [
    "# === 模块 0.5：从文件加载 seed words ===\n",
    "def load_seed_words(path: str = 'seed_words.txt') -> list:\n",
    "\n",
    "    seeds = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            w = line.strip()\n",
    "            if w:\n",
    "                seeds.append(w)\n",
    "\n",
    "    return list(dict.fromkeys(seeds))\n",
    "\n",
    "seed = load_seed_words('seed_words.txt')\n",
    "print(f'已加载 {len(seed)} 个种子词：', seed[:10], '…')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6769c82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading MD&A in : 100%|██████████| 1/1 [00:00<00:00, 11650.84it/s]\n",
      "Loading MD&A in 2022: 100%|██████████| 5069/5069 [00:01<00:00, 3399.64it/s]\n",
      "Loading MD&A in 2023: 100%|██████████| 5254/5254 [00:01<00:00, 4092.44it/s]\n",
      "Loading MD&A in 2019: 100%|██████████| 3789/3789 [00:01<00:00, 3782.66it/s]\n",
      "Loading MD&A in 2021: 100%|██████████| 4662/4662 [00:01<00:00, 4182.03it/s]\n",
      "Loading MD&A in 2020: 100%|██████████| 4228/4228 [00:01<00:00, 3809.76it/s]\n",
      "Tokenizing documents: 100%|██████████| 11219/11219 [54:32<00:00,  3.43it/s] \n"
     ]
    }
   ],
   "source": [
    "# 提取 MD&A 并分词\n",
    "mda_texts = load_reports_mda('./2000-2023MDA文本按年份/')\n",
    "tokenized = segment_documents(mda_texts, model_name='web')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29bdf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "# np.save('my_file.npy', tokenized) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65eaac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = np.load(\"my_file.npy\", allow_pickle=True)\n",
    "\n",
    "tokenized = loaded.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94e999d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building static embeddings: 100%|██████████| 250002/250002 [00:00<00:00, 4831963.45token/s]\n",
      "PMI filtering: 100%|██████████| 905/905 [00:00<00:00, 1976.63it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer, wp_emb = build_static_embeddings(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
    "\n",
    "expanded_sims = expand_seed_with_pmi_multi(\n",
    "    seed,\n",
    "    tokenized,\n",
    "    tokenizer,\n",
    "    wp_emb,\n",
    "    topn=250,\n",
    "    center_k=120,\n",
    "    per_seed_k=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d27166b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完整扩充词典已保存至 expanded_dictionary.csv\n"
     ]
    }
   ],
   "source": [
    "expanded_df = pd.DataFrame(expanded_sims, columns=['word', 'similarity'])\n",
    "expanded_df.to_csv('expanded_dictionary.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"完整扩充词典已保存至 expanded_dictionary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009d9775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算 IDF\n",
    "idf_dict = compute_idf(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4234fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动赋权\n",
    "dict_weights = assign_weights_auto(expanded_sims, idf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7870b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算 AI 得分\n",
    "ai_scores = compute_ai_scores(tokenized, dict_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d6f8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已生成 ai_scores_2023_split.csv，包含 code, name, year, AI_score 四列。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame({\n",
    "\t'Ticker': list(ai_scores.keys()),\n",
    "\t'AI_score': list(ai_scores.values())\n",
    "})\n",
    "\n",
    "split_cols = df['Ticker'].str.split('-', expand=True)\n",
    "split_cols.columns = ['code', 'name', 'year']\n",
    "\n",
    "\n",
    "df = pd.concat([split_cols, df['AI_score']], axis=1)\n",
    "\n",
    "df['year'] = df['year'].astype(int)\n",
    "df['AI_score'] = df['AI_score'].astype(float)\n",
    "df['code'] = df['code'].str.zfill(6)\n",
    "\n",
    "df.to_csv('ai_scores_2023_split.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"已生成 ai_scores_2023_split.csv，包含 code, name, year, AI_score 四列。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9d6cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def compute_total_ai_score(df: pd.DataFrame, base_year: int = None) -> pd.DataFrame:\n",
    "\n",
    "\n",
    "    if base_year is None:\n",
    "        base_year = df['year'].min()\n",
    "\n",
    "    df['weight'] = df['year'] - base_year + 1\n",
    "\n",
    "    stock_scores = (\n",
    "        df\n",
    "        .groupby(['code', 'name'])\n",
    "        .apply(lambda g: (g['AI_score'] * g['weight']).sum() / g['weight'].sum())\n",
    "        .reset_index(name='total_AI_score')\n",
    "    )\n",
    "    return stock_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5073615e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/47/__ycwxq97bs034pbshr5sc400000gn/T/ipykernel_87232/2511184046.py:18: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['AI_score'] * g['weight']).sum() / g['weight'].sum())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stock_scores = compute_total_ai_score(df)\n",
    "\n",
    "top100 = stock_scores.sort_values('total_AI_score', ascending=False).head(100)\n",
    "\n",
    "stock_scores.to_csv('stock_total_ai_score.csv', index=False, encoding='utf-8-sig')\n",
    "top100.to_csv('top100_ai.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
